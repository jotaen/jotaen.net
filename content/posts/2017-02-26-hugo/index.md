+++
draft = true
title = "Fast, cheap and automated"
subtitle = "Deploying static websites to AWS"
date = "2017-02-26"
tags = ["deployment", "cloud"]
image = "/posts/2017-02-26-hugo/cloud-dashboards.jpg"
id = "e7ywT"
url = "e7ywT/deploying-static-website-to-aws"
aliases = ["e7ywT"]
+++

For the last year I relied on the [Jekyll](https://jekyllrb.com/) static site generator for my blog and hosted it on [Github pages](https://help.github.com/articles/using-jekyll-as-a-static-site-generator-with-github-pages/). This was a super convenient experience, because it’s probably the easiest way to kick off a self hosted website. However, I sat down this weekend and migrated all the content to the [Hugo](https://gohugo.io) static site generator and setup the hosting with AWS S3 and AWS CloudFront.

Why Hugo? It is noticably faster. The compile time with Hugo is nearly instantaneous. I also like that it is less opinionated and allows more flexibility for the project structure. But this is just the background story: The topic of this blogpost is how to setup an automated and reliable deployment to AWS, regardless whether the static files were generated by Hugo, Jekyll or any other tool.

## The ups and downs of AWS

Preamble: AWS is a powerful cloud provider. It is made up of numerous small services that are configured independently but can be combined with each other like a construction kit. Moreover, the hosting of static content on AWS is ridiculously cheap. (My monthly bill is just a few cents for my entire account.)

However, the advanced features of AWS are a downside at the same time: the initial hurdle to get started is high and there is plenty room for error. Keep this in mind:

- Amazon has your credit card number: If you accidently leak your credentials or activate an expensive service, the costs can quickly go through the roof.[^1]
- Permissions and access rights are a complex thing. If you make a mistake, someone can use your S3 bucket to share illegal files.

If you are new to AWS, you are best adviced to take the time and understand everything you do properly – even when this is time consuming and frustrating in the beginning.


# Basic setup

For building and deploying our website, we use the following chain:

1. [**Github**](https://github.com) is the place where all source files of the static website are at home.
2. [**Travis CI**](https://travis-ci.org) builds and pushes the static website on every change.
3. [**AWS S3**](https://aws.amazon.com/s3/) holds the generated static files and serves them to the world.
4. [**AWS CloudFront**](https://aws.amazon.com/cloudfront/) (optional) is the CDN service that speeds up load times even more. It also provides an SSL certificate for HTTPS connections.

So, let’s roll up the sleeves!

## Github

I won’t go into detail about setting up a Github repo here. But let me point out once again the importance of not commiting any credentials whatsoever to your repo. Neither for AWS, nor for any other service that you use. If this happens after all, you must immediately revoke the affected credentials. (Just erasing them from the commit history is not sufficient!)

## Travis CI

Everytime we push something to the repo, Travis will take care of generating the static sites, building the assets and deploying it to our S3 bucket. It executes every build in a clean environment, thus making sure that no artifacts from previous builds happen to make their way into production. Before we connect our Github repo in Travis, we must create a `.travis.yml` file in the project root:

```YAML
language: go

install:
  - go get -v github.com/spf13/hugo
  - pip install --user awscli

script:
  - hugo
  - aws s3 sync public/ s3://YOUR_BUCKET_NAME/ --delete
```

It consists of two blocks. (All commands get executed in the order they are specified.)

1. `install`: Since the binaries for hugo and AWS are not part of the [Travis default environment](https://docs.travis-ci.com/user/ci-environment/), we must install them first.
2. `script`: This is where the actual build happens. Note, that we use the AWS CLI rather then the out-of-the-box [Travis S3 deployment](https://docs.travis-ci.com/user/deployment/s3/), because the latter one doesn’t take care of deleting orphaned files.

In order for the AWS CLI to work, we must provide three [environment variables](http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-environment) in the settings of your Travis project: `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY` and `AWS_DEFAULT_REGION`. Make sure to obfuscate them in the build log! More on these keys later.

We can add additional steps to this configuration as we like. For instance, we can compile our SASS/LESS files by installing a CLI tool for that (e.g. `npm install node-sass`) and then running it in the `script` step.

## AWS basic setup

### IAM (identity and access management)

AWS has powerful mechanisms that allow to setup fine granular permissions. First, we go to IAM and create a new user for programmatic access. It’s fine to not assign a group to this user, in which case he won’t have any rights unless we explicitly set them (which is good). Next, we go over to Travis and set the two environment variables for access key and access secret. (See above.)

### S3 Bucket

S3 is short for simple storage service that can be used to store all kinds of files. S3 has a HTTP based interface, so all file operations are performed with regular HTTP requests. This is the reason why the content of S3 buckets can be exposed to the world wide web so easily.

We create a new bucket and enable static website hosting in the bucket properties. Just give it a (unique) name and choose a region.

### Policies

Unless we don’t specify a policy, neither Travis can upload anything nor can anyone view our content in a web browser. Policies can be attached to all kinds of AWS entities, so we can configure them directly in the properties of our S3 bucket. (“Properties” → “Permissions” → “Edit bucket policy”)

```JSON
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "TravisCI",
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::YOUR_AWS_ACCOUNT_ID:user/TRAVIS_IAM_USER_NAME"
      },
      "Action": "s3:*",
      "Resource": [
        "arn:aws:s3:::BUCKET_NAME",
        "arn:aws:s3:::BUCKET_NAME/*"
      ]
    },
    {
      "Sid": "PublicWebsiteAccess",
        "Effect": "Allow",
      "Principal": "*",
      "Action": ["s3:GetObject"],
      "Resource": ["arn:aws:s3:::BUCKET_NAME/*"]
    }
  ]
}
```

What’s happening here?

- With the first statement block, we grant full read and write access for the specific Travis user, that we created in IAM in the previous step. This is important, because otherwise Travis cannot upload files to our bucket.
- With the second statement block, we grant read permissions to the entire bucket content for everyone. Otherwise we could not serve our files to the world.

Of course you must replace the uppercase parts with your specific information. And again: be sure to fully understand what’s happening here and how [AWS policies work](http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html).

## DNS settings

Ensure that everything is setup and running by triggering a build. When the build was successfull, we can access our website via the S3 endpoint, which looks something like this: `BUCKET_NAME.s3-website-us-east-1.amazonaws.com`. Of course, this isn’t a very nice URL, so you want to configure a CNAME record for your own domain that points to this address. (Consult your DNS provider or domain seller on how to do that.)

Sidenote: It’s not recommended to set CNAME records for root level domains, although this is technically possible. You can setup a CNAME record for `www.example.org`, but you shouldn’t do so for `example.org`.[^2] However, most DNS providers offer the option to redirect the root domain to a subdomain (like `www.example.org`).


# Advanced setup (optional)

## AWS CloudFront and HTTPS 

CloudFront is the CDN service of AWS that can be put ahead of S3. This means that all our content isn’t served directly out of the bucket anymore, but it is cached by additional CDN servers (so-called edges) all around the globe.

The biggest benefit for a smaller website like this blog is not performance. (S3 is usually pretty quick already.) CloudFront gives us the ability to setup a free SSL certificate, which would not be possible with S3 alone. But of course there are downsides: caching can be annoying sometimes, because file changes take much longer to be rolled out. Also, CloudFront is a bit more expensive then S3 (depending on how you use it).[^3]

In order to setup CloudFront, create a Distribution in the AWS Console and enter the following information:

- **Origin**: Choose the public S3 endpoint as origin. This is important, because otherwise the `index.html` fallback for subdirectories won’t work.
- **HTTPS**: Activate HTTPS with a default CloudFront certificate. It’s free and AWS will take care of everything.
- **CNAME**: You must specify all the CNAMEs that you setup
- **Query String Forwarding**: It can be handy to activate this, because you can avoid caching issues for embedded JS/CSS scripts by providing an arbitrary query string in your HTML.
- **TTL** CloudFront respects the original caching headers, but you can override the expiration times with the TTL values.

You don’t need to specify a root object and you don’t need to adjust the S3 permissions/policies either. We can switch between CloudFront and S3 by means of our CNAME DNS entries.

## Provisioning tools

Nowadays, infrastructure can be setup in a modern DevOps fashion with tools like [terraform](https://www.terraform.io/). This is not just cool, but it brings in several benefits like predictability and reproducibility. On the other hand though, this would add another layer of complexity and is most likely overkill for a simple setup like ours. The infrastructure that is described below can be easily maintained via the AWS web interface (aka Cloud Console).



[^1]: Leaked credentials are usually [abused to mine bitcoins](http://blog.joemoreno.com/2014/04/5000-security-breach.html) in the compromised accounts.
[^2]: Setting up a CNAME record for the root domain can [break email delivery on this domain](https://joshstrange.com/why-its-a-bad-idea-to-put-a-cname-record-on-your-root-domain).
[^3]: You can estimate your AWS costs with this [handy calculator](https://calculator.s3.amazonaws.com/index.html).