+++
draft = true
title = "Deploy static websites to AWS"
subtitle = "Fast, cheap and automated"
date = "2017-02-26"
tags = ["deployment", "cloud"]
image = "/posts/"
id = "asdf1"
url = "asdf1/title"
aliases = ["asdf1"]
+++

For the last year my blog was based on the [Jekyll](https://jekyllrb.com/) static site generator and hosted on [Github pages](https://help.github.com/articles/using-jekyll-as-a-static-site-generator-with-github-pages/). This was a super convenient experience, however, I sat down this weekend and migrated all the content to [Hugo](https://gohugo.io) with hosting on AWS S3 and AWS Cloudfront. Hugo is noticably faster, the compile time is nearly instantaneous. I also like that it is less opinionated and allows more flexibility for the project structure. But enough of the background story: The topic of this blogpost is how to setup an automated and reliable deployment to AWS, regardless whether the static files were generated by Jekyll or Hugo.

## The ups and downs of AWS
AWS is a powerful cloud provider. It is made up of numerous small services that can be combined with each other, but are configured independently. Moreover, the hosting of static content on AWS is ridicuously cheap. (I pay just a few cents per month for my entire AWS account.)

The only drawback of AWS is that it is pretty advanced. If you are new to AWS, I advice you to take the time to understand everything that you do properly. This is time consuming and can be frustrating in the beginning. However, you should always keep in mind that Amazon has your credit card: If you accidently leak your credentials or activate an expensive service, the costs can quickly go through the roof.[^1]

Permissions and access rights are another complex thing in AWS. If you make a mistake there, someone could use your S3 bucket to share illegal files.


# Overview

We use this build and deploy chain for 

1. [**Github**](https://github.com) is the place where all source files of the static website are at home.
2. [**Travis CI**](https://travis-ci.org) builds and pushes the static website on every change.
3. [**AWS S3**](https://aws.amazon.com/s3/) holds the generated static files and serves them to the world.
4. [**AWS Cloudfront**](https://aws.amazon.com/cloudfront/) (optional) is the CDN service that speeds up load times even more. It also provides an SSL certificate for HTTPS connections.

So, let’s roll up the sleeves!

## Github

I won’t go into detail about setting up a Github repo here. But let me point out once again the importance of not commiting any credentials whatsoever to your repo. Neither for AWS, nor for any other service that you use. If this happens after all, you must immediately revoke the affected credentials. (Just erasing them from the commit history is not sufficient!)

## Travis CI

Everytime we push something to the repo, Travis will take care of generating the static sites, building the assets and deploying it to our S3 bucket. It executes every build in a clean environment, thus making sure that no artifacts from previous builds happen to make their way into production. Before we connect our Github repo in Travis, we must create a `.travis.yml` file in the project root:

```YAML
language: go

install:
  - go get -v github.com/spf13/hugo
  - pip install --user awscli

script:
  - hugo
  - aws s3 sync public/ s3://YOUR_BUCKET_NAME/ --delete
```

It consists of two blocks. All commands get executed in the order they are specified.

1. `install`: Since the binaries for hugo and AWS are not part of the [Travis default environment](https://docs.travis-ci.com/user/ci-environment/), we must install them first.
2. `script`: This is where the actual build happens. Note, that we use the AWS CLI rather then the out-of-the-box [Travis S3 deployment](https://docs.travis-ci.com/user/deployment/s3/), because the latter one doesn’t take care of deleting orphaned files.

In order for the AWS CLI to work, we must provide two environment variables in the settings of your Travis project: `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`. Make sure to obfuscate them in the build log! More on these keys later.

You can add additional steps to this configuration as you like. For instance, I when you must compile your SASS/LESS files, you probably want to install a CLI tool for that (e.g. via npm) and call it in the `script` step.

## AWS basic setup

### IAM (identity and access management)

AWS has powerful mechanisms that allow to setup fine granular permissions. First, we go to IAM and create a new user for programmatic access. It’s fine to not assign a group to this user, in which case he won’t have any rights unless we explicitly set them (which is good). Next, we go over to Travis and set the two environment variables for access key and access secret. (See above.)

### S3 Bucket

S3 is short for simple storage service that can be used to store all kinds of files. S3 has a HTTP based interface, so all file operations are performed with regular HTTP requests.

We create a new bucket and enable static website hosting in the bucket properties. This is important, because otherwise the `index.html` files won’t work.

### Policies

Unless we don’t specify a policy, neither Travis can upload anything nor can anyone view or content in a web browser. Policies can be attached to all kinds of AWS entities, so we can configure them directly in the S3 properties. (“Permissions” → “Edit bucket policy”)

```JSON
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "TravisCI",
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::YOUR_AWS_ACCOUNT_ID:user/TRAVIS_IAM_USER_NAME"
      },
      "Action": "s3:*",
      "Resource": [
        "arn:aws:s3:::BUCKET_NAME",
        "arn:aws:s3:::BUCKET_NAME/*"
      ]
    },
    {
      "Sid": "PublicWebsiteAccess",
        "Effect": "Allow",
      "Principal": "*",
      "Action": ["s3:GetObject"],
      "Resource": ["arn:aws:s3:::BUCKET_NAME/*"]
    }
  ]
}
```

What’s happening here?

- With the first statement block, we grant full read and write access for the specific Travis user, that we created in IAM in the previous step. This is important, because otherwise Travis cannot upload files to our bucket.
- With the second statement block, we grant read permissions to the entire bucket content for everyone. Otherwise we could not serve our files to the world.

Of course you must replace the uppercase parts with your specific information. And again: be sure to fully understand what’s happening here and how [AWS policies work](http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html).

## DNS settings

When everything is setup and running, we can access our website via the S3 endpoint, which looks something like this: `BUCKET_NAME.s3-website-us-east-1.amazonaws.com`. Of course, this isn’t a very nice URL, so you want to configure a CNAME record for your own domain that points to this address. (Consult your DNS provider or domain seller on how to do that.)

Sidenote: It’s not recommended to set CNAME records for root level domains, although this is technically possible. You can setup a CNAME record for `www.example.org`, but you shouldn’t do so for `example.org`.[^2] However, most DNS provider offer the option to redirect the root domain to a subdomain (like `www.example.org`).

## AWS Cloudflare and HTTPS (optional)

If you 


[^1]: Leaked credentials are usually [abused to mine bitcoins](http://blog.joemoreno.com/2014/04/5000-security-breach.html) in the compromised accounts.
[^2]: Setting up a CNAME record for the root domain can [break email delivery on this domain](https://joshstrange.com/why-its-a-bad-idea-to-put-a-cname-record-on-your-root-domain).